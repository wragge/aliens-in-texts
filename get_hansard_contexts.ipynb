{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import arrow\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from IPython.display import display, HTML, FileLink, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Location of downloaded XML files\n",
    "data_dir = '/Volumes/bigdata/mydata/Hansard/xml/'\n",
    "\n",
    "def get_ids_from_csv(csv_file):\n",
    "    df = pd.read_csv(csv_file, parse_dates=['date'])\n",
    "    ids = []\n",
    "    for url in list(df['speech_url']):\n",
    "        # print(url)\n",
    "        ids.append(re.search(r'\\/(\\d{8}_(reps|senate|REPS|SENATE)_\\d+_\\w+)\\/', url).group(1))\n",
    "    ids = list(set(ids))\n",
    "    return ids\n",
    "\n",
    "def get_texts(csv_file):\n",
    "    ids = get_ids_from_csv(csv_file)\n",
    "    for id in ids:\n",
    "        year = id[:4]\n",
    "        date = arrow.get(id[:8], 'YYYYMMDD').format('YYYY-MM-DD')\n",
    "        house = re.search(r'\\d{8}_((reps|senate|REPS|SENATE))_\\w+', id).group(1).lower()\n",
    "        if house == 'reps':\n",
    "            house = 'hofreps'\n",
    "        file_path = os.path.join(data_dir, house, year, '{}.xml'.format(id))\n",
    "        yield file_path\n",
    "        \n",
    "def get_total_files(csv_file):\n",
    "    '''\n",
    "    Get the total number of files.\n",
    "    '''\n",
    "    return len(get_ids_from_csv(csv_file))\n",
    "        \n",
    "def get_words(string, direction, window):\n",
    "    word_list = re.findall(r'\\w+', string)\n",
    "    try:\n",
    "        if direction == 'before':\n",
    "            words = word_list[0-window:]\n",
    "        elif direction == 'after':\n",
    "            words = word_list[:window]\n",
    "    except IndexError:\n",
    "        words = word_list\n",
    "    return words\n",
    "\n",
    "def get_contexts(filename, term, window):\n",
    "    '''\n",
    "    Although it seems weird to match then tokenise, then get surrounding words,\n",
    "    this seems much quicker than using re to get specific numbers of words.\n",
    "    Using TextBlob for tokenisation seems similar, but unnecessary?\n",
    "    What about just getting all the ngrams and filtering to those with the target in the middle?\n",
    "    '''\n",
    "    contexts = []\n",
    "    with open(filename, 'r') as text_file:\n",
    "        content = text_file.read()\n",
    "        soup = BeautifulSoup(content, 'lxml')\n",
    "        for para in soup.find_all('para', string=re.compile(r'\\b{}\\b'.format(term), re.IGNORECASE)):\n",
    "            content = para.get_text()\n",
    "            matches = re.finditer(r'\\b{}\\b'.format(term), content, re.IGNORECASE)\n",
    "            for match in matches:\n",
    "                start = match.start()\n",
    "                end = match.end()\n",
    "                # Get KWIC string\n",
    "                kwic = content[start - 50:end + 50]\n",
    "                # Get lists of words before and after the term\n",
    "                before = get_words(kwic[:50], 'before', window)\n",
    "                after = get_words(kwic[-50:], 'after', window)\n",
    "                contexts.append((kwic, before, after))\n",
    "    return contexts\n",
    "\n",
    "def save_as_csv(df, term, window):\n",
    "    '''\n",
    "    Save the results as a CSV.\n",
    "    Convert lists of words into a pipe-separated string.\n",
    "    '''\n",
    "    df2 = df.copy()\n",
    "    df2['before'] = df['before'].str.join(sep='|')\n",
    "    df2['after'] = df['after'].str.join(sep='|')\n",
    "    df2 = df2[['id', 'date', 'kwic', 'before', 'after']]\n",
    "    filename = 'hansard-{}-words-{}.csv'.format(term, window)\n",
    "    df2.to_csv(filename, index=False)\n",
    "    display(FileLink(filename))\n",
    "\n",
    "def get_all_contexts(csv_file, term, window=5):\n",
    "    all_contexts = []\n",
    "    total = get_total_files(csv_file)\n",
    "    for text in tqdm_notebook(get_texts(csv_file), total=total):\n",
    "        date = arrow.get(os.path.basename(text)[:8], 'YYYYMMDD').format('YYYY-MM-DD')\n",
    "        house = re.search(r'\\d{8}_((reps|senate|REPS|SENATE))_\\w+', text).group(1).lower()\n",
    "        contexts = get_contexts(text, term, window)\n",
    "        for context in contexts:\n",
    "            all_contexts.append({'id': os.path.basename(text), 'date': date, 'house': house, 'kwic': context[0], 'before': context[1], 'after': context[2]})\n",
    "    df = pd.DataFrame(all_contexts)\n",
    "    save_as_csv(df, term, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c8c199247f546f7af061917af365116",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1237), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='hansard-aliens-words-5.csv' target='_blank'>hansard-aliens-words-5.csv</a><br>"
      ],
      "text/plain": [
       "/Users/tim/mycode/aliens/notebooks/hansard-aliens-words-5.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_all_contexts('hansard_aliens.csv', 'aliens', window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "feaae1029cd24784bfb234cb79626c96",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2751), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='hansard-immigrants-words-5.csv' target='_blank'>hansard-immigrants-words-5.csv</a><br>"
      ],
      "text/plain": [
       "/Users/tim/mycode/aliens/notebooks/hansard-immigrants-words-5.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_all_contexts('hansard_immigrants.csv', 'immigrants', window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
