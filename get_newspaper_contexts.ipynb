{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import arrow\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from IPython.display import display, HTML, FileLink, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_texts(harvest):\n",
    "    data_dir = os.path.join('data', harvest, 'text')\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file[-4:] == '.txt':\n",
    "            yield os.path.join(data_dir, file)\n",
    "            \n",
    "def get_total_files(harvest):\n",
    "    '''\n",
    "    Get the total number of files.\n",
    "    '''\n",
    "    data_dir = os.path.join('data', harvest, 'text')\n",
    "    return len([f for f in os.listdir(data_dir) if f[-4:] == '.txt'])\n",
    "\n",
    "\n",
    "def get_words(string, direction, window):\n",
    "    word_list = re.findall(r'\\w+', string)\n",
    "    try:\n",
    "        if direction == 'before':\n",
    "            words = word_list[0-window:]\n",
    "        elif direction == 'after':\n",
    "            words = word_list[:window]\n",
    "    except IndexError:\n",
    "        words = word_list\n",
    "    return words\n",
    "\n",
    "def get_contexts(filename, term, window):\n",
    "    '''\n",
    "    Although it seems weird to match then tokenise, then get surrounding words,\n",
    "    this seems much quicker than using re to get specific numbers of words.\n",
    "    Using TextBlob for tokenisation seems similar, but unnecessary?\n",
    "    What about just getting all the ngrams and filtering to those with the target in the middle?\n",
    "    '''\n",
    "    contexts = []\n",
    "    with open(filename, 'r') as text_file:\n",
    "        content = text_file.read().replace('\\n', ' ')\n",
    "        matches = re.finditer(r'\\b{}\\b'.format(term), content, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            start = match.start()\n",
    "            end = match.end()\n",
    "            # Get KWIC string\n",
    "            kwic = content[start - 50:end + 50]\n",
    "            # Get lists of words before and after the term\n",
    "            before = get_words(kwic[:50], 'before', window)\n",
    "            after = get_words(kwic[-50:], 'after', window)\n",
    "            contexts.append((kwic, before, after))\n",
    "    return contexts\n",
    "\n",
    "def save_as_csv(df, term, window, harvest):\n",
    "    '''\n",
    "    Save the results as a CSV.\n",
    "    Convert lists of words into a pipe-separated string.\n",
    "    '''\n",
    "    df2 = df.copy()\n",
    "    df2['before'] = df['before'].str.join(sep='|')\n",
    "    df2['after'] = df['after'].str.join(sep='|')\n",
    "    df2 = df2[['article_id', 'newspaper_id', 'date', 'kwic', 'before', 'after']]\n",
    "    filename = 'newspapers-{}-{}-words-{}.csv'.format(harvest, term, window)\n",
    "    df2.to_csv(filename, index=False)\n",
    "    display(FileLink(filename))\n",
    "\n",
    "def get_all_contexts(harvest, term, window=2):\n",
    "    all_contexts = []\n",
    "    total = get_total_files(harvest)\n",
    "    for filename in tqdm_notebook(get_texts(harvest), total=total):\n",
    "        date, newspaper_id, article_id = os.path.basename(filename)[:-4].split('-')\n",
    "        date = arrow.get(date, 'YYYYMMDD').format('YYYY-MM-DD')\n",
    "        contexts = get_contexts(filename, term, window)\n",
    "        for context in contexts:\n",
    "            all_contexts.append({'article_id': article_id, 'newspaper_id': newspaper_id, 'date': date, 'kwic': context[0], 'before': context[1], 'after': context[2]})\n",
    "    df = pd.DataFrame(all_contexts)\n",
    "    save_as_csv(df, term, window, harvest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a890052b67f47dea9a3ec80f83cddb1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=505296), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='newspapers-1548377003-immigrants-words-5.csv' target='_blank'>newspapers-1548377003-immigrants-words-5.csv</a><br>"
      ],
      "text/plain": [
       "/Users/tim/mycode/aliens/notebooks/newspapers-1548377003-immigrants-words-5.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_all_contexts('1548377003', 'immigrants', window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dab2ab12f7b64212b2b41f047adea164",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=187954), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='newspapers-1548372464-aliens-words-5.csv' target='_blank'>newspapers-1548372464-aliens-words-5.csv</a><br>"
      ],
      "text/plain": [
       "/Users/tim/mycode/aliens/notebooks/newspapers-1548372464-aliens-words-5.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_all_contexts('1548372464', 'aliens', window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
