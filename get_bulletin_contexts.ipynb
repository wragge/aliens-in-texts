{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import os\n",
    "import arrow\n",
    "import pandas as pd\n",
    "from tqdm import tqdm_notebook\n",
    "from IPython.display import display, HTML, FileLink, clear_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Path to Bulletin text files\n",
    "# You'll need to git clone or download the repository at: https://github.com/wragge/trove-texts\n",
    "# Then change the path below to point to the Bulletin text files\n",
    "data_dir = '/Volumes/bigdata/mydata/Trove/Bulletin/text'\n",
    "\n",
    "def get_texts():\n",
    "    '''\n",
    "    Generator to loop through filenames.\n",
    "    '''\n",
    "    for file in os.listdir(data_dir):\n",
    "        if file[-4:] == '.txt':\n",
    "            yield os.path.join(data_dir, file)\n",
    "            \n",
    "def get_total_files():\n",
    "    '''\n",
    "    Get the total number of files.\n",
    "    '''\n",
    "    return len([f for f in os.listdir(data_dir) if f[-4:] == '.txt'])\n",
    "\n",
    "\n",
    "def get_words(string, direction, window):\n",
    "    word_list = re.findall(r'\\w+', string)\n",
    "    try:\n",
    "        if direction == 'before':\n",
    "            words = word_list[0-window:]\n",
    "        elif direction == 'after':\n",
    "            words = word_list[:window]\n",
    "    except IndexError:\n",
    "        words = word_list\n",
    "    return words\n",
    "\n",
    "def get_contexts(filename, term, window):\n",
    "    '''\n",
    "    Although it seems weird to match then tokenise, then get surrounding words,\n",
    "    this seems much quicker than using re to get specific numbers of words.\n",
    "    Using TextBlob for tokenisation seems similar, but unnecessary?\n",
    "    What about just getting all the ngrams and filtering to those with the target in the middle?\n",
    "    '''\n",
    "    contexts = []\n",
    "    with open(filename, 'r') as text_file:\n",
    "        content = text_file.read().replace('\\n', ' ')\n",
    "        matches = re.finditer(r'\\b{}\\b'.format(term), content, re.IGNORECASE)\n",
    "        for match in matches:\n",
    "            start = match.start()\n",
    "            end = match.end()\n",
    "            # Get KWIC string\n",
    "            kwic = content[start - 50:end + 50]\n",
    "            # Get lists of words before and after the term\n",
    "            before = get_words(kwic[:50], 'before', window)\n",
    "            after = get_words(kwic[-50:], 'after', window)\n",
    "            contexts.append((kwic, before, after))\n",
    "    return contexts\n",
    "\n",
    "def save_as_csv(df, term, window):\n",
    "    '''\n",
    "    Save the results as a CSV.\n",
    "    Convert lists of words into a pipe-separated string.\n",
    "    '''\n",
    "    df2 = df.copy()\n",
    "    df2['before'] = df['before'].str.join(sep='|')\n",
    "    df2['after'] = df['after'].str.join(sep='|')\n",
    "    df2 = df2[['id', 'date', 'kwic', 'before', 'after']]\n",
    "    filename = 'bulletin-{}-words-{}.csv'.format(term, window)\n",
    "    df2.to_csv(filename, index=False)\n",
    "    display(FileLink(filename))\n",
    "\n",
    "def get_all_contexts(term, window=2):\n",
    "    all_contexts = []\n",
    "    total = get_total_files()\n",
    "    for filename in tqdm_notebook(get_texts(), total=total):\n",
    "        date, id = re.search(r'([0-9\\-]+)\\-(nla\\.obj\\-\\d+).*\\.txt$', filename).groups()\n",
    "        # clear_output(wait=True)\n",
    "        # print(filename)\n",
    "        contexts = get_contexts(filename, term, window)\n",
    "        for context in contexts:\n",
    "            all_contexts.append({'id': id, 'date': date, 'kwic': context[0], 'before': context[1], 'after': context[2]})\n",
    "    df = pd.DataFrame(all_contexts)\n",
    "    save_as_csv(df, term, window)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d7d3449659e4cd5bf021f662eb6f2e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4710), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='bulletin-aliens-words-5.csv' target='_blank'>bulletin-aliens-words-5.csv</a><br>"
      ],
      "text/plain": [
       "/Users/tim/mycode/aliens/notebooks/bulletin-aliens-words-5.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_all_contexts('aliens', window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a080786734434c799fff5aed5dea4789",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=4710), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<a href='bulletin-immigrants-words-5.csv' target='_blank'>bulletin-immigrants-words-5.csv</a><br>"
      ],
      "text/plain": [
       "/Users/tim/mycode/aliens/notebooks/bulletin-immigrants-words-5.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "get_all_contexts('immigrants', window=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
